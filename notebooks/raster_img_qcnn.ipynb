{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic doodle classification using Hybrid-QCNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6ed9d912acd0b9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "This Jupyter notebook implements a Hybrid Quantum Convolutional Neural Network (Hybrid-QCNN) for doodle classification on a reduced portion of the Google's Quick Draw dataset.\n",
    "\n",
    "\n",
    "### Structure\n",
    "The structure of this notebook is as follows:\n",
    "1. [Introduction](#1-introduction): A brief overview of the problem, the approach, and the goals.\n",
    "2. [Data Preprocessing](#2-data-preprocessing): The steps taken to prepare the data for modeling, including any necessary data cleaning, normalization, or feature engineering.\n",
    "3. [Hybrid Modeling](#3-hybrid-modeling): The implementation of the Hybrid-QCNN architecture, including the use of quantum-inspired techniques.\n",
    "4. [Training](#4-training): The training process for the model, including the choice of optimizer, loss function, and hyperparameters.\n",
    "5. [Evaluation](#5-evaluation): The evaluation of the trained model, including metrics used to assess its performance, and any visualizations or results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f60aaa64e763aca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "Doodles are a unique form of artistic expression that have gained popularity in recent years. The Google Quick, Draw! dataset provides a vast repository of doodles that can be used to train and evaluate machine learning models. In this notebook, we implement a Hybrid Quantum Convolutional Neural Network (Hybrid-QCNN) to classify doodles into their respective categories.\n",
    "\n",
    "The main challenge in doodle classification is the high variability and complexity of doodle designs. Traditional deep neural networks have been shown to be effective for image classification tasks, but they may not fully capture the quantum-inspired nature of doodle patterns. To address this issue, we incorporate quantum-inspired techniques into our model, leveraging the power of quantum computing to improve the performance and robustness of our classifier.\n",
    "\n",
    "In this notebook, we will explore the implementation details of our Hybrid-QCNN model, including the data preprocessing steps, the architecture design, and the training process. We will also evaluate the performance of our model.\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] Senokosov et al., [Quantum machine learning for image classification](https://arxiv.org/abs/2304.09224)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fd976dcc99ceb9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's import the required libraries:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c31577fe701b1e38"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.554781Z",
     "start_time": "2024-05-22T17:25:29.758783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.nn import  Module, Conv2d, Linear, Flatten, BatchNorm2d\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from rust_sketch import sketches_to_images\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "N_SAMPLES = 100\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 20\n",
    "\n",
    "manual_seed(SEED)\n",
    "algorithm_globals.random_seed = SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data preprocessing\n",
    "\n",
    "In this section, we will prepare our dataset for training and testing by loading and processing the doodle data from the Google Quick, Draw! dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed62d75b369e329c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Load Dataset\n",
    "\n",
    "We load the camera and phone datasets using Pandas' `read_json` function, specifying that the files are in JSON Lines format with a limited number of rows (1,000) for faster processing. We then drop unnecessary columns and convert the sketch data into images using the `sketches_to_images` function.\n",
    "\n",
    "The `sketches_to_images` function, which is implemented in Rust, is used to convert the sketch data into images. This function takes a batch of sketches as input, along with the old and new sizes, and returns a vector of images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be6accdf286a4c01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "camera_df = pd.read_json('../data/quickdraw/full_simplified_camera.ndjson', lines=True, nrows=200)\n",
    "phone_df = pd.read_json('../data/quickdraw/full_simplified_cell phone.ndjson', lines=True, nrows=200)\n",
    "calculator_df = pd.read_json('../data/quickdraw/full_simplified_calculator.ndjson', lines=True, nrows=200)\n",
    "\n",
    "# Drop unnecessary columns and process drawings\n",
    "for df in [camera_df, phone_df, calculator_df]:\n",
    "    df.drop([\"timestamp\", \"countrycode\", \"key_id\", \"recognized\"], axis=1, inplace=True)\n",
    "    df[\"drawing\"] = sketches_to_images(df.drawing.array, 255, 28)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df = pd.concat([camera_df, phone_df, calculator_df])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.810321Z",
     "start_time": "2024-05-22T17:25:34.556781Z"
    }
   },
   "id": "b952562c28f71b8b",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Split train and test datasets\n",
    "\n",
    "In this step, we split our dataset into training (80%) and testing (20%) sets using the `train_test_split` function from scikit-learn. The `test_size` parameter is set to 0.2, which means that 20% of the data will be used for testing. We also use a random seed (`random_state=SEED`) to ensure reproducibility.\n",
    "\n",
    "Additionally, we apply label encoding to the class labels using the `LabelEncoder` from scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f3db49a17f84552"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split data into train and test sets for camera data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['drawing'], df['word'], test_size=0.2, random_state=SEED)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.826325Z",
     "start_time": "2024-05-22T17:25:34.812321Z"
    }
   },
   "id": "579724e2cf8369e8",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Load train dataset\n",
    "\n",
    "In this step, we convert our training data into tensors using PyTorch and create a dataset and data loader for training.\n",
    "\n",
    "We first convert the `X_train` and `y_train` lists to tensors using `torch.tensor`. We specify the data type as `torch.float32` for the input data and `torch.long` for the class labels, assuming that they are integers. The `-1` in the reshape function is used to automatically determine the batch size. We then create a `TensorDataset` from these tensors and a `DataLoader` to load batches of training data during training. The `batch_size` parameter determines how many samples are loaded at once, and `shuffle=True` ensures that the order of the batches is randomized."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b3213763d945b63"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "# --------------\n",
    "X_train_tensor = torch.tensor(X_train.tolist(), dtype=torch.float32).reshape(-1, 28, 28)\n",
    "y_train_tensor = torch.tensor(y_train.tolist(), dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.874321Z",
     "start_time": "2024-05-22T17:25:34.828320Z"
    }
   },
   "id": "7e1c83d089a2127a",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the shape of our training dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a14af36798b3ed"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([480, 28, 28])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.889324Z",
     "start_time": "2024-05-22T17:25:34.877323Z"
    }
   },
   "id": "a1f0b01ed6881bc8",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Load test dataset\n",
    "\n",
    "We also performed the same process for the test dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe75a465dfb55d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "# --------------\n",
    "X_test_tensor = torch.tensor(X_test.tolist(), dtype=torch.float32).reshape(-1, 28, 28)\n",
    "y_test_tensor = torch.tensor(y_test.tolist(), dtype=torch.long)  # Assuming y_test contains class labels as integers\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.921324Z",
     "start_time": "2024-05-22T17:25:34.891326Z"
    }
   },
   "id": "18d2101a2c30e926",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Hybrid-modeling\n",
    "\n",
    "This section focuses on defining the quantum parts of our hybrid quantum-classical model. The components included here are the quantum feature map and the quantum ansatz, which are crucial for encoding classical data into quantum states and processing these states respectively.\n",
    "\n",
    "**Explanation**:\n",
    "* **Quantum Feature Map**: This function creates a quantum circuit that applies a series of parameterized Rx gates to encode classical data into a quantum state. The parameters (features) correspond to aspects of the input data.\n",
    "* **Quantum Ansatz**: This function constructs a quantum circuit that processes quantum data using a series of parameterized rotations and entangling operations. The weights are used to adjust the rotations, and the pattern of entangling gates (CNOTs) is designed to spread quantum information across the circuit, which can be crucial for learning complex patterns in data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c6d6fe8fa1cac34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from qiskit.circuit import ParameterVector\n",
    "\n",
    "\n",
    "def quantum_feature_map(n: int) -> QuantumCircuit:\n",
    "    circuit = QuantumCircuit(n)\n",
    "    features = ParameterVector(\"x\", n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        circuit.rx(features[i], i)\n",
    "    \n",
    "    return circuit\n",
    "    \n",
    "def quantum_ansatz(n: int) -> QuantumCircuit:\n",
    "    # Creamos un circuito cuántico con n qubits.\n",
    "    circuit = QuantumCircuit(n)\n",
    "    weights = ParameterVector(\"w\", n * 3)\n",
    "    \n",
    "    # Asumiendo que 'weights' es una lista de valores de peso,\n",
    "    # y la longitud es suficiente para aplicar Rz a cada qubit.\n",
    "    for i in range(n):\n",
    "        circuit.rz(weights[i], i)\n",
    "        \n",
    "    # Aplicar más rotaciones, que en la imagen parecen ser Ry\n",
    "    for i in range(n):\n",
    "        circuit.ry(weights[n+i], i)\n",
    "        \n",
    "    for i in range(n):\n",
    "        circuit.rz(weights[2*n+i], i)\n",
    "        \n",
    "    # Aplicar entrelazamiento con puertas CNOT\n",
    "    for i in range(n-1):\n",
    "        circuit.cx(i, i+1)\n",
    "    circuit.cx(n-1, 0)\n",
    "    \n",
    "    return circuit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.937319Z",
     "start_time": "2024-05-22T17:25:34.923325Z"
    }
   },
   "id": "fe0d52d164e4a311",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also define the function that creates the whole parametrized quantum circuit (Feature Map + Ansatz):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ff598aa54e1df2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define and create QNN\n",
    "def create_qnn(n):\n",
    "    feature_map = quantum_feature_map(n)\n",
    "    ansatz = quantum_ansatz(n)\n",
    "    \n",
    "    qc = QuantumCircuit(n)\n",
    "    qc.compose(feature_map, inplace=True)\n",
    "    qc.compose(ansatz, inplace=True)\n",
    "\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters,\n",
    "        input_gradients=True,\n",
    "    )\n",
    "    return qnn, qc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:34.953324Z",
     "start_time": "2024-05-22T17:25:34.939320Z"
    }
   },
   "id": "16edfaf74bba1fb3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the whole quantum circuit:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc2fcdb3fcdc2176"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SamplerQNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m qnn, qc \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_qnn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m qc\u001B[38;5;241m.\u001B[39mdraw(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmpl\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[8], line 11\u001B[0m, in \u001B[0;36mcreate_qnn\u001B[1;34m(n)\u001B[0m\n\u001B[0;32m      8\u001B[0m qc\u001B[38;5;241m.\u001B[39mcompose(ansatz, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m qnn \u001B[38;5;241m=\u001B[39m \u001B[43mSamplerQNN\u001B[49m(\n\u001B[0;32m     12\u001B[0m     circuit\u001B[38;5;241m=\u001B[39mqc,\n\u001B[0;32m     13\u001B[0m     input_params\u001B[38;5;241m=\u001B[39mfeature_map\u001B[38;5;241m.\u001B[39mparameters,\n\u001B[0;32m     14\u001B[0m     weight_params\u001B[38;5;241m=\u001B[39mansatz\u001B[38;5;241m.\u001B[39mparameters,\n\u001B[0;32m     15\u001B[0m     input_gradients\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m qnn, qc\n",
      "\u001B[1;31mNameError\u001B[0m: name 'SamplerQNN' is not defined"
     ]
    }
   ],
   "source": [
    "qnn, qc = create_qnn(5)\n",
    "qc.draw(\"mpl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T17:25:35.207321Z",
     "start_time": "2024-05-22T17:25:34.955323Z"
    }
   },
   "id": "689ffee7365ed3d",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "This neural network, defined in the `Net` class, integrates classical convolutional layers with a quantum neural network layer, leveraging the strengths of both classical and quantum computing paradigms. The structure is designed to process image data, commonly used in tasks such as doodle classification.\n",
    "\n",
    "### Components of the Network\n",
    "\n",
    "#### Classical Convolutional Layers\n",
    "- **First Convolutional Layer (`conv1`)**:\n",
    "  - **Channels**: Processes single-channel input (e.g., grayscale images) and outputs 16 feature maps.\n",
    "  - **Kernel Size**: Uses a 5x5 kernel to capture spatial hierarchies.\n",
    "  - **Padding**: Applies padding of 2 to preserve spatial dimensions after convolution.\n",
    "  - **Batch Normalization (`bn1`)**: Normalizes the output of `conv1` to improve training stability and speed.\n",
    "\n",
    "- **Second Convolutional Layer (`conv2`)**:\n",
    "  - Increases the depth to 32 feature maps while maintaining the spatial dimensions, similar to `conv1`.\n",
    "\n",
    "#### Pooling Layers\n",
    "- **Max Pooling**: Applied after each convolutional layer with a kernel size of 2x2, reducing the spatial dimensions by half. This operation helps in reducing the computational complexity and overfitting by abstracting the higher-level features.\n",
    "\n",
    "#### Hybrid Dense Layers\n",
    "- **Flattening**: Converts the multidimensional feature maps into a 1D feature vector necessary for the dense layer.\n",
    "- **First Fully Connected Layer (`fc1`)**:\n",
    "  - Transforms the flattened output into an intermediate space of 5 features.\n",
    "\n",
    "#### Quantum Layer\n",
    "- **Quantum Neural Network (`qnn`)**: Integrated via `TorchConnector`, this layer allows for quantum computations within the network flow, potentially capturing complex patterns that are difficult for classical layers.\n",
    "\n",
    "#### Output Processing\n",
    "- **Second Fully Connected Layer (`fc2`)**: Processes the quantum layer's output to produce a single output feature.\n",
    "- **Concatenation**: The final layer output is concatenated with its complement (`1 - x`) to ensure the output sums to one, often used in binary classification tasks.\n",
    "\n",
    "### Forward Pass\n",
    "- The input passes sequentially through `conv1`, `bn1`, and a ReLU activation, followed by max pooling, repeating this process for `conv2`.\n",
    "- After processing through both convolutional layers and pooling, the data is flattened and passed through `fc1` with a ReLU activation.\n",
    "- The quantum layer receives the processed classical data, applies quantum operations, and passes the result to `fc2`.\n",
    "- The final output is adjusted by concatenating it with its complement, preparing it for output tasks such as classification.\n",
    "\n",
    "### Discussion\n",
    "This architecture effectively combines classical deep learning methods for robust feature extraction with the exploratory capabilities of quantum computing. The design aims to experiment with how quantum layers can enhance classical prediction models, particularly in complex image classification scenarios."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d22a74619c51ac35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.nn import BatchNorm1d\n",
    "\n",
    "\n",
    "# Define torch NN module\n",
    "class Net(Module):\n",
    "    def __init__(self, qnn, num_classes=2):\n",
    "        super().__init__()\n",
    "        # Classical convolutional layer\n",
    "        self.conv1 = Conv2d(\n",
    "            in_channels=1, \n",
    "            out_channels=16, \n",
    "            kernel_size=5,\n",
    "            padding=2\n",
    "        )\n",
    "        self.bn1 = BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = Conv2d(\n",
    "            in_channels=16, \n",
    "            out_channels=32, \n",
    "            kernel_size=5,\n",
    "            padding=2\n",
    "        )\n",
    "        self.bn2 = BatchNorm2d(32)\n",
    "        \n",
    "        # Hybrid dense layers\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Linear(in_features=32 * 7 * 7, out_features=5)\n",
    "        \n",
    "        # Quantum layer\n",
    "        self.qnn = TorchConnector(qnn)\n",
    "        self.fc2 = Linear(2**5, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        x = F.relu(self.fc1(self.flatten(x))) # Batch normalization\n",
    "        \n",
    "        x = self.qnn(x)\n",
    "        x = self.fc2(x) # Batch normalization\n",
    "        \n",
    "        # Apply softmax for multi-class classification\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Net(qnn, num_classes=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T17:25:35.209327Z"
    }
   },
   "id": "50c1ed0933d660ca",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Training\n",
    "\n",
    "This section details the training process for our hybrid quantum-classical neural network. The network utilizes both classical and quantum layers to optimize performance on the classification task.\n",
    "\n",
    "We use the Adam optimizer, known for its efficiency in handling sparse gradients and adaptive learning rate capabilities. The learning rate is set through the `LEARNING_RATE` variable, allowing control over how much the model weights are adjusted during training.\n",
    "The Negative Log Likelihood Loss (`NLLLoss`) is used as the loss function. It is suitable for classification problems with `C` classes when using a log-softmax layer as the final output, which must be the case here.\n",
    "\n",
    "Training occurs over `N_EPOCHS`, allowing the model to iteratively learn from the entire dataset. The training data is processed in batches using `train_loader`. This helps in managing memory usage and allows the optimizer to update model parameters iteratively, refining the learning after each batch. Each batch of data is fed forward through the model (`model(data.unsqueeze(0))`) to compute the predicted outputs. The `unsqueeze(0)` might be used to adjust the dimensions of the input data to match the model's expected input shape (if necessary). The loss between the predicted outputs and the actual targets (`target`) is calculated. This value represents how well the model's predictions match the actual labels. The backward pass computes the gradient of the loss function with respect to the model parameters (`loss.backward()`). This step is crucial for learning as it indicates how each parameter should be adjusted to minimize the loss. The optimizer updates the model parameters based on the gradients calculated during the backward pass (`optimizer.step()`). The loss for each batch is recorded, and the average loss per epoch is calculated and stored in `loss_list`. This average loss is printed after each epoch to monitor the training progress.\n",
    "\n",
    "After each epoch, the training progress is outputted as a percentage of completion along with the average loss. This feedback is valuable for monitoring the training process and determining when the model has sufficiently learned or if adjustments need to be made to training parameters like the learning rate or the number of epochs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f580317fbb91b7a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define optimizer and loss functions\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = CrossEntropyLoss()\n",
    "\n",
    "# Start training\n",
    "epochs = N_EPOCHS\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), desc=\"Training Batches\", leave=False):\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        output = model(data.unsqueeze(0))\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize weights\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "    train_loss_list.append(total_loss / len(train_loader))\n",
    "    train_acc_list.append(100 * correct / total)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data.unsqueeze(0))\n",
    "            loss = loss_func(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "    \n",
    "    val_loss_list.append(val_loss / len(test_loader))\n",
    "    val_acc_list.append(100 * val_correct / val_total)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] \"\n",
    "          f\"Train Loss: {train_loss_list[-1]:.4f} \"\n",
    "          f\"Train Acc: {train_acc_list[-1]:.2f}% \"\n",
    "          f\"Val Loss: {val_loss_list[-1]:.4f} \"\n",
    "          f\"Val Acc: {val_acc_list[-1]:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T17:25:35.211325Z"
    }
   },
   "id": "e3ce5b50217b6e83",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how the model performed over training iterations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80e4c232e0393bfd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plotting the metrics\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_loss_list, label='Train Loss')\n",
    "plt.plot(range(1, epochs + 1), val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_acc_list, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_acc_list, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5b7c11318e8189c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we saved our trained model for future usage."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efdbe81e367d46c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T17:25:35.214324Z"
    }
   },
   "id": "8207063879edb96e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "After training, the next crucial step is to evaluate the model's performance using the test dataset. The evaluation process is crucial to assess the generalization ability of the model on unseen data.\n",
    "\n",
    "The trained model weights are loaded from \"model.pt\", ensuring that the model used for evaluation has the trained parameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df76f0782b46b4b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qnn1, _ = create_qnn(5)\n",
    "model1 = Net(qnn1)\n",
    "model1.load_state_dict(torch.load(\"model.pt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T17:25:35.216324Z"
    }
   },
   "id": "45cba6df13deae0d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting the model to eval() mode is crucial as it disables dropout and batch normalization during the inference, ensuring consistent performance across different test inputs. \n",
    "\n",
    "We perform inference under the no_grad() context to prevent PyTorch from calculating gradients, which are unnecessary during evaluation and only consume extra memory and computing power. For each batch, the model's predictions are compared against the actual labels to compute the number of correct predictions and the loss. These metrics provide insight into the model's effectiveness. Finally, the average loss and accuracy over all test data are computed and printed, providing a straightforward assessment of the model's performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e98d34384329396"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model1.eval()  # set model to evaluation mode\n",
    "with no_grad():\n",
    "\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model1(data.unsqueeze(1))\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = loss_func(output.squeeze(-1), target)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "            sum(total_loss) / len(total_loss), correct / len(test_loader) / BATCH_SIZE * 100\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a13eb6fc4e65268",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This block of code visualizes the predictions for a subset of the test dataset. Images along with their predicted labels (\"Camera\" or \"Cell Phone\") are displayed. This visualization helps in understanding how the model is performing on individual examples."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab0f69995ede1047"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot predicted labels\n",
    "\n",
    "n_samples_show = 24\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(40, 3))\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = model(data.unsqueeze(1))\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap=\"gray\")\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title(\"{}\".format(\"Camera\" if pred.item() == 0 else \"Cell Phone\"))\n",
    "\n",
    "        count += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e115283e4b8a27d0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Through these steps, the model's performance is quantitatively evaluated and qualitatively visualized, providing a comprehensive assessment of its ability to generalize from training to real-world scenarios."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3bbe356f768cc4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
